import os
import sys
import re
import pandas as pd
from sentence_transformers import SentenceTransformer, util
import requests
from tqdm import tqdm
from fastapi import FastAPI, File, UploadFile, Request
from fastapi.responses import JSONResponse
import uvicorn
from typing import List, Dict, Any, Optional

# --- –õ–æ–≥–≥–µ—Ä ---
class Logger(object):
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, "w", encoding="utf-8")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        self.terminal.flush()
        self.log.flush()

sys.stdout = Logger("log_output.txt")

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
MODEL_ID = "deepseek/deepseek-chat-v3-0324:free"
OPENROUTER_API_KEY = "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"
SIMILARITY_THRESHOLD = 0.85  # –ü–æ–≤—ã—à–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–±–æ—Ä–∞

print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å SentenceTransformer...")
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø—É—Ç–µ–π
BASE_PKL_PATH = os.getenv("BASE_PKL_PATH", "products_with_embeddings.pkl")

def download_embeddings_if_needed():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ"""
    if os.path.exists(BASE_PKL_PATH):
        return True
    
    download_url = os.getenv("EMBEDDINGS_DOWNLOAD_URL")
    if not download_url:
        return False
    
    try:
        print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ {download_url}")
        
        # –î–ª—è Google Drive –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(download_url, stream=True, headers=headers)
        response.raise_for_status()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        content_type = response.headers.get('content-type', '')
        if 'text/html' in content_type or response.text[:100].strip().startswith('<'):
            print("‚ùå –ü–æ–ª—É—á–µ–Ω–∞ HTML —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤–º–µ—Å—Ç–æ —Ñ–∞–π–ª–∞. Google Drive —Ç—Ä–µ–±—É–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.")
            return False
        
        total_size = int(response.headers.get('content-length', 0))
        
        with open(BASE_PKL_PATH, 'wb') as file, tqdm(
            desc="–ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
            total=total_size,
            unit='iB',
            unit_scale=True,
            unit_divisor=1024,
        ) as pbar:
            for data in response.iter_content(chunk_size=1024):
                size = file.write(data)
                pbar.update(size)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω
        if os.path.getsize(BASE_PKL_PATH) < 1000:  # –ú–µ–Ω—å—à–µ 1KB - —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ HTML
            print("‚ùå –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π, –≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ HTML —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
            os.remove(BASE_PKL_PATH)
            return False
        
        print(f"‚úÖ –§–∞–π–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω: {os.path.getsize(BASE_PKL_PATH) / (1024*1024):.1f} MB")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        if os.path.exists(BASE_PKL_PATH):
            os.remove(BASE_PKL_PATH)
        return False

print("üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")

# –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
if not os.path.exists(BASE_PKL_PATH):
    download_embeddings_if_needed()

if os.path.exists(BASE_PKL_PATH):
    df_base: pd.DataFrame = pd.read_pickle(BASE_PKL_PATH)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df_base)} —Å—Ç—Ä–æ–∫ —Å –≥–æ—Ç–æ–≤—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏.")
else:
    print(f"‚ö†Ô∏è –§–∞–π–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ {BASE_PKL_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.")
    # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
    df_base = pd.DataFrame({
        'sku': [],
        'name': [],
        'category': [],
        'emb': []
    })
    print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –ø—É—Å—Ç–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö. –°–µ—Ä–≤–∏—Å –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ, –Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞.")

# === –ó–ê–ü–†–û–° –í OpenRouter ===
def query_openrouter(prompt: str, model: str = MODEL_ID) -> Optional[str]:
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://yourdomain.com",
        "X-Title": "CategoryMatcher"
    }
    body = {
        "model": model,
        "messages": [
            {"role": "system", "content": "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–æ–≤–∞—Ä–∞ –ø–æ –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—é."},
            {"role": "user", "content": prompt}
        ]
    }

    response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=body)

    try:
        data = response.json()
    except Exception as e:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON:", str(e))
        print("üîÅ Response content:", response.text)
        return None

    if response.status_code != 200:
        print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç OpenRouter:", data.get("error", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞"))
        return None

    if "choices" not in data:
        print("‚ö†Ô∏è –í –æ—Ç–≤–µ—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á 'choices':", data)
        return None

    return data["choices"][0]["message"]["content"].strip()

# === API –§—É–Ω–∫—Ü—ñ—ó ===
app = FastAPI(title="AI Category Matcher", description="API –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤")

@app.get("/")
async def root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ API"""
    return {
        "message": "AI Category Matcher API",
        "version": "1.0.0",
        "endpoints": {
            "/process": "POST - –û–±—Ä–∞–±–æ—Ç–∫–∞ Excel —Ñ–∞–π–ª–∞",
            "/n8n_process": "POST - –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è n8n",
            "/health": "GET - –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"
        }
    }

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {"status": "healthy", "model_loaded": True, "base_loaded": len(df_base) > 0}

@app.post("/process")
async def process_file(file: UploadFile = File(...)):
    """–û–±—Ä–æ–±–ª—è—î Excel —Ñ–∞–π–ª —á–µ—Ä–µ–∑ API"""
    try:
        # –ß–∏—Ç–∞—î–º–æ –≤—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª
        df_new = pd.read_excel(await file.read())
        results = process_products(df_new)
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É JSON
        return JSONResponse({
            "status": "success",
            "data": results,
            "processed_items": len(results)
        })
    except Exception as e:
        return JSONResponse(
            {"status": "error", "message": str(e)},
            status_code=400
        )

@app.post("/n8n_process")
async def n8n_process(request: Request):
    """–°–ø–µ—Ü—ñ–∞–ª—å–Ω–∏–π –µ–Ω–¥–ø–æ—ñ–Ω—Ç –¥–ª—è n8n"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–∞—Ä—Å–∏–º –≤—Ä—É—á–Ω—É—é
        raw_data = await request.json()
        print("üì¶ –ü–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ:", raw_data)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ —É DataFrame
        processed_data = {
            "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": raw_data.get("name") or raw_data.get("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ") or raw_data.get("title") or "",
            "–ê—Ä—Ç–∏–∫—É–ª": raw_data.get("code") or raw_data.get("–ê—Ä—Ç–∏–∫—É–ª") or raw_data.get("sku") or ""
        }
        
        print("üîç –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:", processed_data)
        
        if not processed_data["–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"]:
            return JSONResponse(
                {"status": "error", "message": "–ü–æ–ª–µ '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ' –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ"},
                status_code=400
            )
            
        df_new = pd.DataFrame([processed_data])
        results = process_products(df_new)
        
        return JSONResponse({
            "status": "success",
            "data": results[0] if results else {},
            "processed_items": len(results)
        })
    except Exception as e:
        return JSONResponse(
            {"status": "error", "message": str(e)},
            status_code=400
        )

def process_products(df_new: pd.DataFrame) -> List[Dict[str, Any]]:
    """–û—Å–Ω–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Ç–æ–≤–∞—Ä—ñ–≤"""
    global df_base, model
    results = []
    try:
        records = df_new.to_dict(orient="records")
        if not records:
            raise ValueError("–ü—É—Å—Ç–æ–π DataFrame")
        for idx, row in enumerate(tqdm(records, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞")):
            name = str(row.get("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "")).strip()
            art = str(row.get("–ê—Ä—Ç–∏–∫—É–ª", "")).strip()
            if not name:
                results.append({
                    "status": "error",
                    "message": "–ü–æ–ª–µ '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ' –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ",
                    "data": None
                })
                continue
            print(f"\nüÜï [{idx+1}/{len(records)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–≤–∞—Ä: {name}")
            # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ –ø–µ—Ä—à–∏–º 2-3 —Å–ª–æ–≤–∞–º
            product_words = name.lower().split()[:3]
            product_type = " ".join(product_words)
            escaped_words = [re.escape(word) for word in product_words]
            # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ name –∫ —Å—Ç—Ä–æ–∫–µ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫ Pandas
            df_base["name"] = df_base["name"].astype(str)
            mask = df_base["name"].str.lower().str.contains('|'.join(escaped_words), regex=True)
            df_filtered: pd.DataFrame = df_base[mask].copy()
            if df_filtered.empty or len(df_filtered) < 5:
                print(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —Ç–∏–ø—É —Ç–æ–≤–∞—Ä–∞ '{product_type}', –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—é –±–∞–∑—É.")
                df_filtered = df_base.copy()
            else:
                print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(df_filtered)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —Ç–∏–ø—É '{product_type}'")
            # –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Å—Ö–æ–∂–æ—Å—Ç—ñ
            emb = model.encode(name, convert_to_tensor=True)
            df_filtered["similarity"] = df_filtered["emb"].apply(lambda x: util.cos_sim(emb, x).item())
            df_filtered = df_filtered.sort_values("similarity", ascending=False)
            # –õ–æ–≥–∏–∫–∞ –≤—ã–±–æ—Ä–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            top_matches = df_filtered[df_filtered["similarity"] >= SIMILARITY_THRESHOLD]
            # --- –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º (–∫–∞–∫ –±—ã–ª–æ)
            cat_ai, confidence, sku_ai = "", "", ""
            if top_matches.empty:
                lower_threshold = 0.75
                top_matches = df_filtered[df_filtered["similarity"] >= lower_threshold]
                print(f"‚ö†Ô∏è –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ {SIMILARITY_THRESHOLD:.2f}, –ø—Ä–æ–±—É–µ–º –ø–æ—Ä–æ–≥ {lower_threshold:.2f}")
                if top_matches.empty:
                    lower_threshold = 0.65
                    top_matches = df_filtered[df_filtered["similarity"] >= lower_threshold]
                    print(f"‚ö†Ô∏è –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, –ø—Ä–æ–±—É–µ–º –ø–æ—Ä–æ–≥ {lower_threshold:.2f}")
                    if top_matches.empty:
                        top_matches = df_filtered.head(1)
                        print(f"‚ö†Ô∏è –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–∞–∂–µ –ø—Ä–∏ –ø–æ—Ä–æ–≥–µ {lower_threshold:.2f}, –±–µ—Ä–µ–º –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç")
                        if df_filtered.iloc[0]["similarity"] < 0.5:
                            cat_ai = "–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å"
                            confidence = "–Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"
                            sku_ai = ""
                            print("–ö–∞—Ç–µ–≥–æ—Ä–∏—è: –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å (–∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è < 0.5)")
            else:
                top_variants = "\n".join([
                    f"{i+1}. {match['category']} ({match['name']}) ‚Äî {match['similarity']*100:.1f}% [SKU: {match['sku']}]"
                    for i, match in enumerate(top_matches.to_dict(orient="records"))
                ])
                print("üîç –¢–æ–ø —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π:")
                print(top_variants)
                # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—É—é –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                best = top_matches.iloc[0]
                cat_ai = best["category"]
                confidence = f"{best['similarity']*100:.1f}"
                sku_ai = best["sku"]

            # --- –í—Å–µ–≥–¥–∞ –¥–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ AI ---
            print("\nü§ñ –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —É AI...")
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–ª–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è prompt
            category_chain = cat_ai if cat_ai else ""
            category_structured = ""
            if category_chain:
                parts = [c.strip() for c in category_chain.split('>')]
                for i, part in enumerate(parts):
                    exclam = '!' * i
                    category_structured += f"category{exclam}: {part}\n"
                category_chain_block = f"\n–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è —Ç–æ–≤–∞—Ä–∞:\n{category_structured}\n"
            else:
                category_chain_block = ""
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–æ–ø –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –¥–ª—è prompt (—É—Å—Ç–æ–π—á–∏–≤–æ –∫ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—é top_variants)
            top_variants_str = top_variants if 'top_variants' in locals() else ''
            # –§–æ—Ä–º–∏—Ä—É–µ–º prompt
            ai_prompt = f'''–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ç–µ—Ö–Ω–∏–∫–µ –∏ –∑–∞–ø—á–∞—Å—Ç—è–º —Å 20-–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–æ–≤–∞—Ä–∞ –ø–æ –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—é.\n\n–ü—Ä–∞–≤–∏–ª–∞ –∞–Ω–∞–ª–∏–∑–∞:\n1. –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: \"{name}\"\n2. –û–ø—Ä–µ–¥–µ–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∏–ø —Ç–µ—Ö–Ω–∏–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: –º–æ—Ç–æ–±–ª–æ–∫, –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, —Å–∫—É—Ç–µ—Ä –∏ —Ç.–¥.)\n3. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∏–∂–µ –∏ –≤—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ\n4. –ï—Å–ª–∏ —Å–æ–º–Ω–µ–≤–∞–µ—à—å—Å—è (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å < 95%) ‚Äî —É–∫–∞–∂–∏ \"–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å\"\n5. –î–ª—è —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ‚â• 99.9%) —É–∫–∞–∂–∏ —Ç–æ—á–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É–∫–∞–∑–∞–Ω–∏—è:\n- –£—á–∏—Ç—ã–≤–∞–π —Å–ø–µ—Ü–∏—Ñ–∏–∫—É —Ç–æ–≤–∞—Ä–∞ (–∑–∞–ø—á–∞—Å—Ç—å, –∞–∫—Å–µ—Å—Å—É–∞—Ä, –∫–æ–º–ø–ª–µ–∫—Ç –∏ —Ç.–¥.)\n- –ü—Ä–æ–≤–µ—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –±—Ä–µ–Ω–¥–∞ –∏ –º–æ–¥–µ–ª–∏\n- –ò–≥–Ω–æ—Ä–∏—Ä—É–π –æ–±—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –ø–æ–ª—å–∑—É –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö\n{category_chain_block}\n–¢–æ–ø –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤:\n{top_variants_str}\n\n–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (—Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π):\ncategory: [—É—Ä–æ–≤–µ–Ω—å 1]\ncategory!: [—É—Ä–æ–≤–µ–Ω—å 2]\ncategory!!: [—É—Ä–æ–≤–µ–Ω—å 3]\n...\n–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: [—Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ 50-100, % —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏]\n\n–ü—Ä–∏–º–µ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞:\ncategory: –ú–æ—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏\ncategory!: –ê–º–æ—Ä—Ç–∏–∑–∞—Ç–æ—Ä—ã –∏ –ø–æ–¥–≤–µ—Å–∫–∞\ncategory!!: –ê–º–æ—Ä—Ç–∏–∑–∞—Ç–æ—Ä—ã –∑–∞–¥–Ω–∏–µ\n–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: 99\n'''
            ai_category = query_openrouter(ai_prompt)
            print(f"AI –æ—Ç–≤–µ—Ç: {ai_category}")

            results.append({
                "–ê—Ä—Ç–∏–∫—É–ª": art,
                "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": name,
                "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": cat_ai,
                "SKU": sku_ai,
                "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π": confidence,
                "AI_–ö–∞—Ç–µ–≥–æ—Ä–∏—è": ai_category
            })
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ process_products: {str(e)}")
        raise
    return results

if __name__ == "__main__":
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä—Ç –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è (–¥–ª—è –æ–±–ª–∞—á–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º)
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")  # 0.0.0.0 –¥–ª—è –æ–±–ª–∞—á–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º
    
    print("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º AI Category Matcher API...")
    print(f"üì° API –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: http://{host}:{port}")
    print(f"üîó –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è n8n: http://{host}:{port}/n8n_process")
    print(f"üè• –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è: http://{host}:{port}/health")
    
    # –û—Ç–∫–ª—é—á–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ Uvicorn
    uvicorn.run(app, host=host, port=port, log_config=None)
